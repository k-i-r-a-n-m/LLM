{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb362472-b673-4c1a-bae2-e6a716466e91",
   "metadata": {},
   "source": [
    "# RAG Implementation Using\n",
    "-  Pinecone(managed Vector DB)\n",
    "-  OpenAI (embedding model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f0ac6c-e179-44e9-8b88-26023d981dc5",
   "metadata": {},
   "source": [
    "## Ingesting the data to the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee1e3ff-297b-49e6-b273-24265061afbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# 1. Load the Documents (Text file)\n",
    "loader = TextLoader('./Data/mediumBlog.txt', encoding='UTF-8')\n",
    "document = loader.load()\n",
    "\n",
    "#2.split it into chunks\n",
    "print(\"splitting...\")\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(document)\n",
    "\n",
    "print(f\"created {len(texts)} chunks\")\n",
    "\n",
    "#3.Create Embedding form the splitted text documnt\n",
    "\n",
    "#Embedding model Initialization\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "\n",
    "#4. Process and Store in Vector DB\n",
    "print(\"Ingestion....\")\n",
    "PineconeVectorStore.from_documents(texts, embeddings, index_name=os.environ[\"INDEX_NAME\"])\n",
    "print(\"##--Finish--##\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f164fc-83d3-4a3c-b915-d85537abfc2f",
   "metadata": {},
   "source": [
    "## RAG using Chain constructors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf3b5cad-973b-4138-8044-9ce4d09c6a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving...\n",
      "--------Normal Response without RAG------\n",
      "I'm sorry, but without the specific blog post or blog URL, I am unable to provide the exact date when it was published.\n",
      "--------using RAG--------\n",
      "The blog was published on December 22, 2023.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain import hub\n",
    "\n",
    "#New imports related to RAG\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Retrieving...\")\n",
    "#Embedd the Input query\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "# query = 'who is the author of the medium blog post related to vector database?'\n",
    "# query = 'what did Chip Huen quoted'\n",
    "query = 'when was the blog published?'\n",
    "prompt = PromptTemplate.from_template(template=query)\n",
    "chain = prompt | llm\n",
    "result = chain.invoke({})\n",
    "print(\"--------Normal Response without RAG------\")\n",
    "print(result.content)\n",
    "\n",
    "vectorStore = PineconeVectorStore(embedding=embeddings,index_name=os.environ[\"INDEX_NAME\"])\n",
    "\n",
    "\n",
    "#Download PROMPT for RETRIEVAL\n",
    "retrieval_prompt = hub.pull('langchain-ai/retrieval-qa-chat')\n",
    "\n",
    "#Format list of document into a prompt and pass to llm\n",
    "combine_doc_chain = create_stuff_documents_chain(llm,retrieval_prompt)\n",
    "\n",
    "#Retrieve data from the VectorStore (Retriver)\n",
    "retrieval_chain = create_retrieval_chain(retriever=vectorStore.as_retriever(), combine_docs_chain=combine_doc_chain)\n",
    "\n",
    "#invoke the retrieval chain\n",
    "result = retrieval_chain.invoke(input={\"input\":query})\n",
    "\n",
    "print(\"--------using RAG--------\")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f14af6-a9d8-466f-bb73-122d2720134d",
   "metadata": {},
   "source": [
    "## RAG using LCEL chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "655956a4-3ecb-4219-b75d-f61afb600676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving...\n",
      "The author of the article is Ejiro Onose. Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain import hub\n",
    "\n",
    "#New imports related to RAG\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Function formats the document list from vector db -- same as (create_stuff_documents_chain)\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "print(\"Retrieving...\")\n",
    "llm = ChatOpenAI()\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# After ingesting the data --> Access the vectorDB using their index_name in DB\n",
    "vectorStore = PineconeVectorStore(embedding=embeddings,index_name=os.environ[\"INDEX_NAME\"])\n",
    "    \n",
    "    \n",
    "#Using LCEL to build the RAG chain without chain constructor (create_stuff_document_chain & create_retrieval_chain)\n",
    "template = \"\"\"\n",
    "    Use the following pieces of context to answer the question at the end.\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    Use three sentences maximum and keep the answer as concise as possible. \n",
    "    Always say \"thanks for asking!\" at the end of the.\n",
    "  \n",
    "    {context}\n",
    "    \n",
    "    Question:{question}\n",
    "    \n",
    "    Helpful Answer:\n",
    "    \"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(template=template)\n",
    "\n",
    "rag_chain = ({\"context\":vectorStore.as_retriever() | format_docs,\"question\":RunnablePassthrough()}\n",
    "             | custom_rag_prompt\n",
    "             | llm)\n",
    "\n",
    "result = rag_chain.invoke(\"get me author name\")\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ad8a81-2543-4966-8fa8-b6f2d9081342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
