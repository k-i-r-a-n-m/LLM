{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac0e9312-292a-4c09-bb6d-819c1c6d47ad",
   "metadata": {},
   "source": [
    "# Build a Document Chat Bot using Langchain(RAG technique)-StreamLit(UI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efe2bfe-35fb-450e-ab6b-210458862806",
   "metadata": {},
   "source": [
    "## Ingestion of Data to VectorDB - Pinecone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35005dc0-ef1e-4f0e-85c8-1cd73a932b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import ReadTheDocsLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# separators = [\"\\n\\n\", \"\\n\", \" \", \"\"])  --> recursively\n",
    "def ingest_docs():\n",
    "    file_path = os.path.abspath(\"./data/langchain-docs-old\")\n",
    "    embedding = OpenAIEmbeddings()\n",
    "    index_name = os.environ.get(\"INDEX_NAME\")\n",
    "\n",
    "    #Document loader\n",
    "    loader = ReadTheDocsLoader(path=file_path, encoding=\"UTF-8\")\n",
    "    raw_document = loader.load()\n",
    "\n",
    "    #initializing text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"])\n",
    "\n",
    "    documents = text_splitter.split_documents(documents=raw_document)\n",
    "    print(f\"Splitted into {len(documents)} chunks\")\n",
    "\n",
    "    print(f\"Inserting {len(documents)} chunks to pinecone...\")\n",
    "    PineconeVectorStore.from_documents(documents, index_name=index_name, embedding=embedding)\n",
    "    print(f\"########---Embedded {len(documents)} chunks to pinecone---#######\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ingest_docs()\n",
    "\n",
    "    # llm = ChatOpenAI(temperature=0,max_tokens=\"50\")\n",
    "    # llm.invoke(\"what is langchain? explain in simple terms!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf3bc27-1f6e-44d3-8458-e86e758bc725",
   "metadata": {},
   "source": [
    "## Core LLM Function Call with query and Chat History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21a44b8-fc9c-48da-8db7-1449f91998d8",
   "metadata": {},
   "source": [
    "![image1](./images/conversational_retrieval_chain.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8b89794-3054-4a91-9cac-1ebf5a9ab40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'LangChain was built by Harrison Chase.',\n",
      " 'chat_history': [],\n",
      " 'context': [Document(page_content='Get Started with LangChain in Node.js by Developers Digest\\nLangChain + OpenAI tutorial: Building a Q&A system w/ own text data by Samuel Chan\\nLangchain + Zapier Agent by Merk\\nConnecting the Internet with ChatGPT (LLMs) using Langchain And Answers Your Questions by Kamalraj M M\\nBuild More Powerful LLM Applications for Business’s with LangChain (Beginners Guide) by No Code Blackbox\\nprevious\\nTracing\\n Contents\\n  \\nIntroduction to LangChain with Harrison Chase, creator of LangChain\\nTutorials\\nVideos (sorted by views)\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 25, 2023.', metadata={'source': 'D:\\\\Python\\\\LLM\\\\doc-assistant\\\\data\\\\langchain-docs-old\\\\langchain.readthedocs.io\\\\en\\\\latest\\\\youtube.html'}),\n",
      "             Document(page_content=\"by Sam, a successful entrepreneur, who is working on a hackathon project with Deven to add more complex memory structures to Langchain.'}\", metadata={'source': 'D:\\\\Python\\\\LLM\\\\doc-assistant\\\\data\\\\langchain-docs-old\\\\langchain.readthedocs.io\\\\en\\\\latest\\\\modules\\\\memory\\\\types\\\\entity_summary_memory.html'}),\n",
      "             Document(page_content='go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\\\\n\\\\nð\\\\x9f“\\\\x9a Data Augmented Generation:\\\\n\\\\nData Augmented Generation involves specific types of chains that first interact with an external datasource to fetch data to use in the generation step. Examples of this include summarization of long pieces of text and question/answering over specific data sources.\\\\n\\\\nð\\\\x9f¤\\\\x96 Agents:\\\\n\\\\nAgents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\\\\n\\\\nð\\\\x9f§\\\\xa0 Memory:\\\\n\\\\nMemory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a', metadata={'source': 'D:\\\\Python\\\\LLM\\\\doc-assistant\\\\data\\\\langchain-docs-old\\\\langchain.readthedocs.io\\\\en\\\\latest\\\\modules\\\\indexes\\\\document_loaders\\\\examples\\\\markdown.html'}),\n",
      "             Document(page_content=\"[Document(page_content='LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an API, but will also:\\\\nBe data-aware: connect a language model to other sources of data\\\\nBe agentic: allow a language model to interact with its environment\\\\nThe LangChain framework is designed with the above principles in mind.\\\\nThis is the Python specific portion of the documentation. For a purely conceptual guide to LangChain, see here. For the JavaScript documentation, see here.\\\\nGetting Started\\\\nCheckout the below guide for a walkthrough of how to get started using LangChain to create an Language Model application.\\\\nGetting Started Documentation\\\\nModules\\\\nThere are several main modules that LangChain provides support for. For each module we provide some examples to get started, how-to guides, reference docs, and conceptual guides. These modules are, in increasing\", metadata={'source': 'D:\\\\Python\\\\LLM\\\\doc-assistant\\\\data\\\\langchain-docs-old\\\\langchain.readthedocs.io\\\\en\\\\latest\\\\modules\\\\indexes\\\\document_loaders\\\\examples\\\\diffbot.html'})],\n",
      " 'input': 'who built the langchain?'}\n"
     ]
    }
   ],
   "source": [
    "# filePath --> package(backend/core.py)\n",
    "\n",
    "import os\n",
    "\n",
    "from langchain import hub\n",
    "from dotenv import load_dotenv\n",
    "from typing import Any,List,Tuple\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from  langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "import pprint as pp\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "def run_llm(query:str,chat_history:List[Tuple]=[]) -> Any:\n",
    "    # Initializing\n",
    "    llm = ChatOpenAI(temperature=0)\n",
    "    embedding = OpenAIEmbeddings()\n",
    "    retrieval_prompt = hub.pull('langchain-ai/retrieval-qa-chat')\n",
    "    rephrase_prompt = hub.pull(\"langchain-ai/chat-langchain-rephrase\")\n",
    "\n",
    "    # Initializing VectorDB\n",
    "    vector_store = PineconeVectorStore.from_existing_index(index_name=os.environ[\"CHAT_ASSISTANT_PINECONE_INDEX_NAME\"],embedding=embedding)\n",
    "\n",
    "    # Chains\n",
    "    combine_docs_chain = create_stuff_documents_chain(llm,retrieval_prompt)\n",
    "\n",
    "    # LLM Build's the query from the provided (query+chatHistory) --> summarizing the chat history and build a query based on it\n",
    "    history_aware_retreiver_chain = create_history_aware_retriever(\n",
    "        llm, vector_store.as_retriever(), rephrase_prompt\n",
    "    )\n",
    "\n",
    "    # retrieval_chain = create_retrieval_chain(retriever=vector_store.as_retriever(),combine_docs_chain=combine_docs_chain)\n",
    "    \n",
    "    question_answer_chain = create_retrieval_chain(\n",
    "                                             history_aware_retreiver_chain,\n",
    "                                             combine_docs_chain=combine_docs_chain)\n",
    "\n",
    "    result = question_answer_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "# ----- Call's \n",
    "# chat_history = [(\"human\",\"who created langchain?\"),(\"ai\",\"harrison chase created it!\")]\n",
    "# run_llm(\"tell me more about him\", chat_history)\n",
    "pp.pprint(run_llm(\"who built the langchain?\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cf9948-9b15-4e04-8e8e-c5c8c98d56fc",
   "metadata": {},
   "source": [
    "## Connecting the LLM with streamlit UI  (streamlit run main.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f103195-757d-4152-a686-356dcccff39d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filePath: main.py\n",
    "# run using streamlit run main.py --> serves a webpage\n",
    "\n",
    "import streamlit as st\n",
    "from streamlit_chat import message\n",
    "\n",
    "# own import\n",
    "# from backend.core import run_llm\n",
    "\n",
    "\n",
    "# streamlit will always rerun the UI .... creating session_state --> helps to maintain the data on the UI bewteen re-renders \n",
    "if \"user_prompt_history\" not in st.session_state:\n",
    "    st.session_state[\"user_prompt_history\"] = []\n",
    "\n",
    "if \"chat_answer_history\" not in st.session_state:\n",
    "    st.session_state[\"chat_answer_history\"] = []\n",
    "\n",
    "if \"chat_history\" not in st.session_state:\n",
    "    st.session_state[\"chat_history\"] = []\n",
    "\n",
    "\n",
    "st.header(\"Langchain(v1) -- Documentation Helper Bot\")\n",
    "prompt = st.text_input(\"Prompt\",placeholder=\"Enter your prompt here..\")\n",
    "\n",
    "if prompt:\n",
    "    with st.spinner(\"Generating responses...\"):\n",
    "        # import time\n",
    "        # time.sleep(3)\n",
    "        generated_response = run_llm(query=prompt, chat_history = st.session_state[\"chat_history\"])\n",
    "        formatted_answer = f\"\\n{generated_response[\"answer\"]}\"\n",
    "        st.session_state[\"user_prompt_history\"].append(prompt)\n",
    "        st.session_state[\"chat_answer_history\"].append(formatted_answer)\n",
    "        for x in [(\"human\", prompt), (\"ai\", formatted_answer)]:\n",
    "            st.session_state[\"chat_history\"].append(x)\n",
    "        print(generated_response)\n",
    "\n",
    "if st.session_state[\"chat_answer_history\"]:\n",
    "    for user_query, generated_response in zip(st.session_state[\"user_prompt_history\"], st.session_state[\"chat_answer_history\"]):\n",
    "        message(user_query, is_user=True)\n",
    "        message(generated_response)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b37ad1-3370-49e4-9350-f8c8c60b0898",
   "metadata": {},
   "source": [
    "#### Initializing Streamlit -- not working -- run in command line with correct Virtual environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddce59b3-7e87-4fae-a764-5e5a087db21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install streamlit_jupyter\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "from streamlit_jupyter import StreamlitPatcher, tqdm\n",
    "\n",
    "StreamlitPatcher().jupyter()  # register streamlit with jupyter-compatible wrappers\n",
    "st.header(\"Title from streamlit\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
